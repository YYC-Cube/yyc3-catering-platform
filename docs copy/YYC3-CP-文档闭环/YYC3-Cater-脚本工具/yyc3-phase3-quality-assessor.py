#!/usr/bin/env python3
"""
YYCÂ³ æ–‡æ¡£è´¨é‡è¯„ä¼°å·¥å…· - ç¬¬ä¸‰é˜¶æ®µï¼ˆP2ï¼‰
å¤šç»´åº¦è¯„ä¼°æ–‡æ¡£è´¨é‡ï¼Œæä¾›æ”¹è¿›å»ºè®®
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import json
from dataclasses import dataclass, field
from collections import Counter


@dataclass
class DocumentQualityMetrics:
    """æ–‡æ¡£è´¨é‡æŒ‡æ ‡"""
    completeness: float = 0.0  # å®Œæ•´æ€§
    accuracy: float = 0.0  # å‡†ç¡®æ€§
    readability: float = 0.0  # å¯è¯»æ€§
    practicality: float = 0.0  # å®ç”¨æ€§
    consistency: float = 0.0  # ä¸€è‡´æ€§
    overall_score: float = 0.0  # ç»¼åˆè¯„åˆ†
    
    # è¯¦ç»†æŒ‡æ ‡
    has_title: bool = False
    has_description: bool = False
    has_author: bool = False
    has_version: bool = False
    has_table_of_contents: bool = False
    has_code_examples: bool = False
    has_best_practices: bool = False
    has_case_studies: bool = False
    has_faq: bool = False
    has_references: bool = False
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_sections: int = 0
    code_blocks: int = 0
    code_lines: int = 0
    total_lines: int = 0
    word_count: int = 0
    avg_section_length: float = 0.0


@dataclass
class QualityIssue:
    """è´¨é‡é—®é¢˜"""
    severity: str  # critical, major, minor, info
    category: str  # completeness, accuracy, readability, practicality, consistency
    message: str
    suggestion: str
    line_number: Optional[int] = None


@dataclass
class DocumentQualityReport:
    """æ–‡æ¡£è´¨é‡è¯„ä¼°æŠ¥å‘Š"""
    file_path: str
    file_name: str
    doc_type: str
    metrics: DocumentQualityMetrics
    issues: List[QualityIssue] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)
    grade: str = "F"  # A, B, C, D, F


class DocumentQualityAssessor:
    """æ–‡æ¡£è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.reports: List[DocumentQualityReport] = []
        
        # æ ‡å‡†ç« èŠ‚åˆ—è¡¨
        self.standard_sections = [
            "æ¦‚è¿°", "ç®€ä»‹", "èƒŒæ™¯", "ç›®æ ‡", "èŒƒå›´",
            "æ ¸å¿ƒæ¦‚å¿µ", "åŸºæœ¬æ¦‚å¿µ", "æœ¯è¯­", "å®šä¹‰",
            "å®æ–½æ­¥éª¤", "å®ç°æ­¥éª¤", "æ“ä½œæŒ‡å—", "ä½¿ç”¨æ–¹æ³•",
            "ä»£ç ç¤ºä¾‹", "ç¤ºä¾‹ä»£ç ", "ä»£ç å®ç°",
            "æ³¨æ„äº‹é¡¹", "æ³¨æ„äº‹é¡¹", "é™åˆ¶", "çº¦æŸ",
            "æœ€ä½³å®è·µ", "å®è·µå»ºè®®", "ç»éªŒæ€»ç»“",
            "å¸¸è§é—®é¢˜", "FAQ", "é—®é¢˜è§£ç­”",
            "æ¡ˆä¾‹åˆ†æ", "æ¡ˆä¾‹ç ”ç©¶", "å®é™…æ¡ˆä¾‹",
            "æ€»ç»“", "æ€»ç»“ä¸å±•æœ›", "ç»“è®º",
            "å‚è€ƒæ–‡æ¡£", "ç›¸å…³æ–‡æ¡£", "å‚è€ƒèµ„æ–™"
        ]
        
        # æ–‡æ¡£ç±»å‹æƒé‡
        self.doc_type_weights = {
            "architecture": {
                "completeness": 0.25,
                "accuracy": 0.30,
                "readability": 0.15,
                "practicality": 0.20,
                "consistency": 0.10
            },
            "technique": {
                "completeness": 0.20,
                "accuracy": 0.20,
                "readability": 0.20,
                "practicality": 0.30,
                "consistency": 0.10
            },
            "template": {
                "completeness": 0.30,
                "accuracy": 0.20,
                "readability": 0.15,
                "practicality": 0.25,
                "consistency": 0.10
            }
        }
    
    def detect_doc_type(self, file_path: Path) -> str:
        """æ£€æµ‹æ–‡æ¡£ç±»å‹"""
        file_name = file_path.name.lower()
        
        if "æ¶æ„ç±»" in file_name:
            return "architecture"
        elif "æŠ€å·§ç±»" in file_name:
            return "technique"
        elif "æ¨¡ç‰ˆ" in file_name or "æ¨¡æ¿" in file_name:
            return "template"
        else:
            return "technique"  # é»˜è®¤
    
    def extract_metadata(self, content: str) -> Dict[str, bool]:
        """æå–æ–‡æ¡£å…ƒæ•°æ®"""
        metadata = {
            "has_title": False,
            "has_description": False,
            "has_author": False,
            "has_version": False,
            "has_table_of_contents": False
        }
        
        # æ£€æŸ¥æ ‡é¢˜
        if re.search(r'^#\s+.+', content, re.MULTILINE):
            metadata["has_title"] = True
        
        # æ£€æŸ¥æè¿°
        if re.search(r'@description|æè¿°|è¯´æ˜', content):
            metadata["has_description"] = True
        
        # æ£€æŸ¥ä½œè€…
        if re.search(r'@author|ä½œè€…', content):
            metadata["has_author"] = True
        
        # æ£€æŸ¥ç‰ˆæœ¬
        if re.search(r'@version|ç‰ˆæœ¬|v\d+\.\d+\.\d+', content):
            metadata["has_version"] = True
        
        # æ£€æŸ¥ç›®å½•
        if re.search(r'ç›®å½•|TOC|##\s+\d+\.', content):
            metadata["has_table_of_contents"] = True
        
        return metadata
    
    def count_sections(self, content: str) -> int:
        """ç»Ÿè®¡ç« èŠ‚æ•°é‡"""
        # ç»Ÿè®¡äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰
        sections = re.findall(r'^##\s+.+', content, re.MULTILINE)
        return len(sections)
    
    def count_code_blocks(self, content: str) -> Tuple[int, int]:
        """ç»Ÿè®¡ä»£ç å—æ•°é‡å’Œä»£ç è¡Œæ•°"""
        # ç»Ÿè®¡ä»£ç å—
        code_blocks = re.findall(r'```[\s\S]*?```', content)
        block_count = len(code_blocks)
        
        # ç»Ÿè®¡ä»£ç è¡Œæ•°
        code_lines = 0
        for block in code_blocks:
            lines = block.split('\n')
            # æ’é™¤ä»£ç å—æ ‡è®°è¡Œ
            code_lines += len([l for l in lines if l and not l.startswith('```')])
        
        return block_count, code_lines
    
    def check_standard_sections(self, content: str) -> Dict[str, bool]:
        """æ£€æŸ¥æ ‡å‡†ç« èŠ‚æ˜¯å¦å­˜åœ¨"""
        sections_found = {}
        
        for section in self.standard_sections:
            # æ£€æŸ¥ç« èŠ‚æ ‡é¢˜
            pattern = rf'^##\s*.*{section}.*$'
            if re.search(pattern, content, re.MULTILINE | re.IGNORECASE):
                sections_found[section] = True
            else:
                sections_found[section] = False
        
        return sections_found
    
    def assess_completeness(self, content: str, metrics: DocumentQualityMetrics) -> float:
        """è¯„ä¼°å®Œæ•´æ€§"""
        score = 0.0
        max_score = 100.0
        
        # å…ƒæ•°æ®å®Œæ•´æ€§ (30åˆ†)
        metadata_score = 0
        if metrics.has_title:
            metadata_score += 6
        if metrics.has_description:
            metadata_score += 6
        if metrics.has_author:
            metadata_score += 6
        if metrics.has_version:
            metadata_score += 6
        if metrics.has_table_of_contents:
            metadata_score += 6
        score += metadata_score
        
        # ç« èŠ‚å®Œæ•´æ€§ (40åˆ†)
        sections = self.check_standard_sections(content)
        sections_score = sum(sections.values()) / len(sections) * 40
        score += sections_score
        
        # å†…å®¹å®Œæ•´æ€§ (30åˆ†)
        content_score = 0
        if metrics.has_code_examples:
            content_score += 10
        if metrics.has_best_practices:
            content_score += 10
        if metrics.has_case_studies:
            content_score += 5
        if metrics.has_faq:
            content_score += 5
        score += content_score
        
        return score / max_score
    
    def assess_accuracy(self, content: str, metrics: DocumentQualityMetrics) -> float:
        """è¯„ä¼°å‡†ç¡®æ€§"""
        score = 0.0
        max_score = 100.0
        
        # æ£€æŸ¥ä»£ç ç¤ºä¾‹ (40åˆ†)
        if metrics.has_code_examples and metrics.code_blocks > 0:
            score += 40
        elif metrics.code_blocks > 0:
            score += 20
        
        # æ£€æŸ¥æŠ€æœ¯å‡†ç¡®æ€§ (30åˆ†)
        # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚
        if re.search(r'(API|æ¥å£|å‡½æ•°|ç±»|æ–¹æ³•|å‚æ•°|è¿”å›å€¼)', content):
            score += 15
        if re.search(r'(ç¤ºä¾‹|ä¾‹å­|demo|Demo)', content):
            score += 15
        
        # æ£€æŸ¥ç‰ˆæœ¬ä¿¡æ¯ (30åˆ†)
        if metrics.has_version:
            score += 30
        
        return score / max_score
    
    def assess_readability(self, content: str, metrics: DocumentQualityMetrics) -> float:
        """è¯„ä¼°å¯è¯»æ€§"""
        score = 0.0
        max_score = 100.0
        
        # æ®µè½é•¿åº¦ (20åˆ†)
        paragraphs = content.split('\n\n')
        avg_para_length = sum(len(p) for p in paragraphs) / len(paragraphs) if paragraphs else 0
        if 100 <= avg_para_length <= 500:
            score += 20
        elif 50 <= avg_para_length < 100 or 500 < avg_para_length <= 800:
            score += 10
        
        # æ ‡é¢˜å±‚çº§ (20åˆ†)
        headers = re.findall(r'^#+\s+.+', content, re.MULTILINE)
        if len(headers) >= 5:
            score += 20
        elif len(headers) >= 3:
            score += 10
        
        # åˆ—è¡¨ä½¿ç”¨ (20åˆ†)
        lists = re.findall(r'^\s*[-*+]\s+', content, re.MULTILINE)
        if len(lists) >= 10:
            score += 20
        elif len(lists) >= 5:
            score += 10
        
        # è¡¨æ ¼ä½¿ç”¨ (20åˆ†)
        tables = re.findall(r'\|.*\|', content)
        if len(tables) >= 3:
            score += 20
        elif len(tables) >= 1:
            score += 10
        
        # ä»£ç æ³¨é‡Š (20åˆ†)
        if metrics.code_blocks > 0:
            score += 20
        
        return score / max_score
    
    def assess_practicality(self, content: str, metrics: DocumentQualityMetrics) -> float:
        """è¯„ä¼°å®ç”¨æ€§"""
        score = 0.0
        max_score = 100.0
        
        # ä»£ç ç¤ºä¾‹ (30åˆ†)
        if metrics.has_code_examples:
            if metrics.code_blocks >= 3:
                score += 30
            elif metrics.code_blocks >= 1:
                score += 15
        
        # æœ€ä½³å®è·µ (30åˆ†)
        if metrics.has_best_practices:
            score += 30
        
        # æ¡ˆä¾‹åˆ†æ (20åˆ†)
        if metrics.has_case_studies:
            score += 20
        
        # å¸¸è§é—®é¢˜ (20åˆ†)
        if metrics.has_faq:
            score += 20
        
        return score / max_score
    
    def assess_consistency(self, content: str, metrics: DocumentQualityMetrics) -> float:
        """è¯„ä¼°ä¸€è‡´æ€§"""
        score = 0.0
        max_score = 100.0
        
        # å‘½åä¸€è‡´æ€§ (30åˆ†)
        # æ£€æŸ¥æœ¯è¯­æ˜¯å¦ä¸€è‡´
        terms = re.findall(r'\b[A-Z][a-zA-Z]+\b', content)
        term_counts = Counter(terms)
        # å¦‚æœæœ‰é‡å¤çš„å¤§å†™æœ¯è¯­ï¼Œè¯´æ˜å‘½åä¸€è‡´
        if term_counts.most_common(1)[0][1] >= 3:
            score += 30
        elif term_counts.most_common(1)[0][1] >= 2:
            score += 15
        
        # æ ¼å¼ä¸€è‡´æ€§ (30åˆ†)
        # æ£€æŸ¥æ ‡é¢˜æ ¼å¼
        headers = re.findall(r'^#+\s+.+', content, re.MULTILINE)
        if len(headers) > 0:
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ç»Ÿä¸€çš„æ ‡é¢˜æ ¼å¼
            consistent = all(re.match(r'^#+\s+', h) for h in headers)
            if consistent:
                score += 30
        
        # ä»£ç é£æ ¼ä¸€è‡´æ€§ (20åˆ†)
        if metrics.code_blocks > 0:
            score += 20
        
        # å…ƒæ•°æ®ä¸€è‡´æ€§ (20åˆ†)
        if metrics.has_title and metrics.has_author and metrics.has_version:
            score += 20
        
        return score / max_score
    
    def calculate_grade(self, overall_score: float) -> str:
        """è®¡ç®—ç­‰çº§"""
        if overall_score >= 90:
            return "A"
        elif overall_score >= 80:
            return "B"
        elif overall_score >= 70:
            return "C"
        elif overall_score >= 60:
            return "D"
        else:
            return "F"
    
    def generate_issues(self, metrics: DocumentQualityMetrics, content: str) -> List[QualityIssue]:
        """ç”Ÿæˆè´¨é‡é—®é¢˜åˆ—è¡¨"""
        issues = []
        
        # å®Œæ•´æ€§é—®é¢˜
        if not metrics.has_title:
            issues.append(QualityIssue(
                severity="critical",
                category="completeness",
                message="æ–‡æ¡£ç¼ºå°‘æ ‡é¢˜",
                suggestion="åœ¨æ–‡æ¡£å¼€å¤´æ·»åŠ  # æ ‡é¢˜"
            ))
        
        if not metrics.has_description:
            issues.append(QualityIssue(
                severity="major",
                category="completeness",
                message="æ–‡æ¡£ç¼ºå°‘æè¿°ä¿¡æ¯",
                suggestion="æ·»åŠ  @description æˆ–æè¿°ç« èŠ‚"
            ))
        
        if not metrics.has_author:
            issues.append(QualityIssue(
                severity="major",
                category="completeness",
                message="æ–‡æ¡£ç¼ºå°‘ä½œè€…ä¿¡æ¯",
                suggestion="æ·»åŠ  @author æˆ–ä½œè€…ç« èŠ‚"
            ))
        
        if not metrics.has_version:
            issues.append(QualityIssue(
                severity="major",
                category="completeness",
                message="æ–‡æ¡£ç¼ºå°‘ç‰ˆæœ¬ä¿¡æ¯",
                suggestion="æ·»åŠ  @version æˆ–ç‰ˆæœ¬ç« èŠ‚"
            ))
        
        if not metrics.has_table_of_contents:
            issues.append(QualityIssue(
                severity="minor",
                category="completeness",
                message="æ–‡æ¡£ç¼ºå°‘ç›®å½•",
                suggestion="æ·»åŠ ç›®å½•ç« èŠ‚ï¼Œæå‡æ–‡æ¡£å¯å¯¼èˆªæ€§"
            ))
        
        # å®ç”¨æ€§é—®é¢˜
        if not metrics.has_code_examples:
            issues.append(QualityIssue(
                severity="major",
                category="practicality",
                message="æ–‡æ¡£ç¼ºå°‘ä»£ç ç¤ºä¾‹",
                suggestion="æ·»åŠ ä»£ç ç¤ºä¾‹ï¼Œæå‡æ–‡æ¡£å®ç”¨æ€§"
            ))
        
        if not metrics.has_best_practices:
            issues.append(QualityIssue(
                severity="major",
                category="practicality",
                message="æ–‡æ¡£ç¼ºå°‘æœ€ä½³å®è·µ",
                suggestion="æ·»åŠ æœ€ä½³å®è·µç« èŠ‚ï¼Œåˆ†äº«ç»éªŒæ€»ç»“"
            ))
        
        if not metrics.has_case_studies:
            issues.append(QualityIssue(
                severity="minor",
                category="practicality",
                message="æ–‡æ¡£ç¼ºå°‘æ¡ˆä¾‹åˆ†æ",
                suggestion="æ·»åŠ æ¡ˆä¾‹åˆ†æç« èŠ‚ï¼Œæä¾›å®é™…åº”ç”¨åœºæ™¯"
            ))
        
        if not metrics.has_faq:
            issues.append(QualityIssue(
                severity="minor",
                category="practicality",
                message="æ–‡æ¡£ç¼ºå°‘å¸¸è§é—®é¢˜",
                suggestion="æ·»åŠ å¸¸è§é—®é¢˜ç« èŠ‚ï¼Œè§£ç­”ç”¨æˆ·ç–‘é—®"
            ))
        
        # å‡†ç¡®æ€§é—®é¢˜
        if metrics.code_blocks == 0:
            issues.append(QualityIssue(
                severity="major",
                category="accuracy",
                message="æ–‡æ¡£ç¼ºå°‘ä»£ç å—",
                suggestion="æ·»åŠ ä»£ç å—ï¼Œæä¾›å…·ä½“å®ç°ç¤ºä¾‹"
            ))
        
        # å¯è¯»æ€§é—®é¢˜
        if metrics.total_sections < 5:
            issues.append(QualityIssue(
                severity="minor",
                category="readability",
                message=f"æ–‡æ¡£ç« èŠ‚è¿‡å°‘ï¼ˆä»…{metrics.total_sections}ä¸ªï¼‰",
                suggestion="å¢åŠ æ›´å¤šç« èŠ‚ï¼Œå®Œå–„æ–‡æ¡£ç»“æ„"
            ))
        
        return issues
    
    def generate_suggestions(self, report: DocumentQualityReport) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        metrics = report.metrics
        
        # åŸºäºè¯„åˆ†ç”Ÿæˆå»ºè®®
        if metrics.completeness < 0.7:
            suggestions.append("å®Œå–„æ–‡æ¡£å…ƒæ•°æ®ï¼Œæ·»åŠ æ ‡é¢˜ã€æè¿°ã€ä½œè€…ã€ç‰ˆæœ¬ç­‰ä¿¡æ¯")
            suggestions.append("å¢åŠ æ ‡å‡†ç« èŠ‚ï¼šæ¦‚è¿°ã€æ ¸å¿ƒæ¦‚å¿µã€å®æ–½æ­¥éª¤ã€ä»£ç ç¤ºä¾‹ç­‰")
        
        if metrics.accuracy < 0.7:
            suggestions.append("æ·»åŠ æ›´å¤šä»£ç ç¤ºä¾‹ï¼Œæå‡æŠ€æœ¯å‡†ç¡®æ€§")
            suggestions.append("æä¾›å…·ä½“çš„APIæ¥å£ã€å‡½æ•°ã€å‚æ•°è¯´æ˜")
        
        if metrics.readability < 0.7:
            suggestions.append("ä¼˜åŒ–æ®µè½é•¿åº¦ï¼Œæ§åˆ¶åœ¨100-500å­—ä¹‹é—´")
            suggestions.append("å¢åŠ åˆ—è¡¨ã€è¡¨æ ¼ç­‰æ ¼å¼ï¼Œæå‡å¯è¯»æ€§")
            suggestions.append("æ·»åŠ æ›´å¤šæ ‡é¢˜å±‚çº§ï¼Œæ”¹å–„æ–‡æ¡£ç»“æ„")
        
        if metrics.practicality < 0.7:
            suggestions.append("æ·»åŠ æœ€ä½³å®è·µç« èŠ‚ï¼Œåˆ†äº«ç»éªŒæ€»ç»“")
            suggestions.append("å¢åŠ æ¡ˆä¾‹åˆ†æï¼Œæä¾›å®é™…åº”ç”¨åœºæ™¯")
            suggestions.append("è¡¥å……å¸¸è§é—®é¢˜ï¼Œè§£ç­”ç”¨æˆ·ç–‘é—®")
        
        if metrics.consistency < 0.7:
            suggestions.append("ç»Ÿä¸€æœ¯è¯­ä½¿ç”¨ï¼Œä¿æŒå‘½åä¸€è‡´æ€§")
            suggestions.append("è§„èŒƒæ ¼å¼ï¼Œä¿æŒæ ‡é¢˜ã€åˆ—è¡¨ç­‰æ ¼å¼ç»Ÿä¸€")
            suggestions.append("ç»Ÿä¸€ä»£ç é£æ ¼ï¼Œä¿æŒä»£ç æ ¼å¼ä¸€è‡´")
        
        return suggestions
    
    def assess_document(self, file_path: Path) -> DocumentQualityReport:
        """è¯„ä¼°å•ä¸ªæ–‡æ¡£"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æµ‹æ–‡æ¡£ç±»å‹
        doc_type = self.detect_doc_type(file_path)
        
        # æå–å…ƒæ•°æ®
        metadata = self.extract_metadata(content)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_sections = self.count_sections(content)
        code_blocks, code_lines = self.count_code_blocks(content)
        total_lines = len(content.split('\n'))
        word_count = len(content.split())
        avg_section_length = total_lines / total_sections if total_sections > 0 else 0
        
        # æ£€æŸ¥æ ‡å‡†ç« èŠ‚
        sections = self.check_standard_sections(content)
        
        # åˆ›å»ºæŒ‡æ ‡å¯¹è±¡
        metrics = DocumentQualityMetrics(
            has_title=metadata["has_title"],
            has_description=metadata["has_description"],
            has_author=metadata["has_author"],
            has_version=metadata["has_version"],
            has_table_of_contents=metadata["has_table_of_contents"],
            has_code_examples=code_blocks > 0,
            has_best_practices=sections.get("æœ€ä½³å®è·µ", False) or sections.get("å®è·µå»ºè®®", False),
            has_case_studies=sections.get("æ¡ˆä¾‹åˆ†æ", False) or sections.get("æ¡ˆä¾‹ç ”ç©¶", False),
            has_faq=sections.get("å¸¸è§é—®é¢˜", False) or sections.get("FAQ", False) or sections.get("é—®é¢˜è§£ç­”", False),
            has_references=sections.get("å‚è€ƒæ–‡æ¡£", False) or sections.get("ç›¸å…³æ–‡æ¡£", False),
            total_sections=total_sections,
            code_blocks=code_blocks,
            code_lines=code_lines,
            total_lines=total_lines,
            word_count=word_count,
            avg_section_length=avg_section_length
        )
        
        # è¯„ä¼°å„ä¸ªç»´åº¦
        metrics.completeness = self.assess_completeness(content, metrics)
        metrics.accuracy = self.assess_accuracy(content, metrics)
        metrics.readability = self.assess_readability(content, metrics)
        metrics.practicality = self.assess_practicality(content, metrics)
        metrics.consistency = self.assess_consistency(content, metrics)
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        weights = self.doc_type_weights.get(doc_type, self.doc_type_weights["technique"])
        metrics.overall_score = (
            metrics.completeness * weights["completeness"] +
            metrics.accuracy * weights["accuracy"] +
            metrics.readability * weights["readability"] +
            metrics.practicality * weights["practicality"] +
            metrics.consistency * weights["consistency"]
        ) * 100
        
        # åˆ›å»ºæŠ¥å‘Š
        report = DocumentQualityReport(
            file_path=str(file_path),
            file_name=file_path.name,
            doc_type=doc_type,
            metrics=metrics,
            grade=self.calculate_grade(metrics.overall_score)
        )
        
        # ç”Ÿæˆé—®é¢˜
        report.issues = self.generate_issues(metrics, content)
        
        # ç”Ÿæˆå»ºè®®
        report.suggestions = self.generate_suggestions(report)
        
        return report
    
    def assess_all_documents(self, directory: Path) -> List[DocumentQualityReport]:
        """è¯„ä¼°ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
        reports = []
        
        for file in directory.rglob("*.md"):
            if file.name == "README.md":
                continue
            
            try:
                report = self.assess_document(file)
                reports.append(report)
                print(f"âœ“ å·²è¯„ä¼°: {file.name} - è¯„åˆ†: {report.metrics.overall_score:.1f} - ç­‰çº§: {report.grade}")
            except Exception as e:
                print(f"âœ— è¯„ä¼°å¤±è´¥: {file.name} - {e}")
        
        return reports
    
    def save_report(self, reports: List[DocumentQualityReport], suffix: str = ""):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = self.base_path / "YYC3-Cater-å®¡æ ¸æŠ¥å‘Š"
        report_dir.mkdir(exist_ok=True)
        
        report_file = report_dir / f"YYC3-æ–‡æ¡£è´¨é‡è¯„ä¼°æŠ¥å‘Š{suffix}.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_documents": len(reports),
                "avg_score": sum(r.metrics.overall_score for r in reports) / len(reports) if reports else 0,
                "grade_distribution": {
                    "A": sum(1 for r in reports if r.grade == "A"),
                    "B": sum(1 for r in reports if r.grade == "B"),
                    "C": sum(1 for r in reports if r.grade == "C"),
                    "D": sum(1 for r in reports if r.grade == "D"),
                    "F": sum(1 for r in reports if r.grade == "F")
                }
            },
            "reports": [
                {
                    "file_path": r.file_path,
                    "file_name": r.file_name,
                    "doc_type": r.doc_type,
                    "metrics": {
                        "completeness": r.metrics.completeness,
                        "accuracy": r.metrics.accuracy,
                        "readability": r.metrics.readability,
                        "practicality": r.metrics.practicality,
                        "consistency": r.metrics.consistency,
                        "overall_score": r.metrics.overall_score
                    },
                    "grade": r.grade,
                    "issues": [
                        {
                            "severity": i.severity,
                            "category": i.category,
                            "message": i.message,
                            "suggestion": i.suggestion
                        }
                        for i in r.issues
                    ],
                    "suggestions": r.suggestions
                }
                for r in reports
            ]
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(reports, report_dir, suffix)
    
    def generate_markdown_report(self, reports: List[DocumentQualityReport], report_dir: Path, suffix: str):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = report_dir / f"YYC3-æ–‡æ¡£è´¨é‡è¯„ä¼°æŠ¥å‘Š{suffix}.md"
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_docs = len(reports)
        avg_score = sum(r.metrics.overall_score for r in reports) / total_docs if reports else 0
        grade_dist = {
            "A": sum(1 for r in reports if r.grade == "A"),
            "B": sum(1 for r in reports if r.grade == "B"),
            "C": sum(1 for r in reports if r.grade == "C"),
            "D": sum(1 for r in reports if r.grade == "D"),
            "F": sum(1 for r in reports if r.grade == "F")
        }
        
        # æŒ‰è¯„åˆ†æ’åº
        sorted_reports = sorted(reports, key=lambda r: r.metrics.overall_score, reverse=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# YYCÂ³ æ–‡æ¡£è´¨é‡è¯„ä¼°æŠ¥å‘Š\n\n")
            f.write(f"**è¯„ä¼°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**è¯„ä¼°ç›®å½•**: {self.base_path}\n\n")
            
            f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»æ–‡æ¡£æ•°**: {total_docs}\n")
            f.write(f"- **å¹³å‡è¯„åˆ†**: {avg_score:.1f}\n")
            f.write(f"- **é€šè¿‡ç‡**: {sum(1 for r in reports if r.metrics.overall_score >= 60) / total_docs * 100:.1f}%\n\n")
            
            f.write("### ç­‰çº§åˆ†å¸ƒ\n\n")
            f.write("| ç­‰çº§ | æ•°é‡ | å æ¯” |\n")
            f.write("|------|------|------|\n")
            for grade in ["A", "B", "C", "D", "F"]:
                count = grade_dist[grade]
                percentage = count / total_docs * 100 if total_docs > 0 else 0
                f.write(f"| {grade} | {count} | {percentage:.1f}% |\n")
            f.write("\n")
            
            f.write("### è¯„åˆ†åˆ†å¸ƒ\n\n")
            f.write("```text\n")
            for report in sorted_reports:
                bar = "â–ˆ" * int(report.metrics.overall_score / 5)
                f.write(f"{report.metrics.overall_score:5.1f} {bar} {report.file_name}\n")
            f.write("```\n\n")
            
            f.write("## ğŸ“ˆ è¯¦ç»†è¯„ä¼°ç»“æœ\n\n")
            for report in sorted_reports:
                f.write(f"### {report.file_name}\n\n")
                f.write(f"**ç­‰çº§**: {report.grade} | **è¯„åˆ†**: {report.metrics.overall_score:.1f}\n\n")
                
                f.write("#### è´¨é‡æŒ‡æ ‡\n\n")
                f.write(f"- **å®Œæ•´æ€§**: {report.metrics.completeness * 100:.1f}%\n")
                f.write(f"- **å‡†ç¡®æ€§**: {report.metrics.accuracy * 100:.1f}%\n")
                f.write(f"- **å¯è¯»æ€§**: {report.metrics.readability * 100:.1f}%\n")
                f.write(f"- **å®ç”¨æ€§**: {report.metrics.practicality * 100:.1f}%\n")
                f.write(f"- **ä¸€è‡´æ€§**: {report.metrics.consistency * 100:.1f}%\n\n")
                
                if report.issues:
                    f.write("#### å‘ç°çš„é—®é¢˜\n\n")
                    for issue in report.issues:
                        emoji = {"critical": "ğŸ”´", "major": "ğŸŸ¡", "minor": "ğŸŸ¢", "info": "ğŸ”µ"}
                        f.write(f"{emoji.get(issue.severity, 'âšª')} **{issue.severity}**: {issue.message}\n")
                        f.write(f"   å»ºè®®: {issue.suggestion}\n\n")
                
                if report.suggestions:
                    f.write("#### æ”¹è¿›å»ºè®®\n\n")
                    for suggestion in report.suggestions:
                        f.write(f"- {suggestion}\n")
                    f.write("\n")
        
        print(f"MarkdownæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='YYCÂ³ æ–‡æ¡£è´¨é‡è¯„ä¼°å·¥å…·')
    parser.add_argument('--base-path', type=str,
                       default='/Users/yanyu/yyc3-catering-platform/docs/YYC3-Cater-Platform-æ–‡æ¡£é—­ç¯',
                       help='æ–‡æ¡£æ ¹ç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("YYCÂ³ æ–‡æ¡£è´¨é‡è¯„ä¼°å·¥å…· - ç¬¬ä¸‰é˜¶æ®µï¼ˆP2ï¼‰")
    print("=" * 80)
    print(f"è¯„ä¼°ç›®å½•: {args.base_path}")
    print("=" * 80)
    print()
    
    assessor = DocumentQualityAssessor(args.base_path)
    reports = assessor.assess_all_documents(Path(args.base_path))
    
    print()
    print("=" * 80)
    print("è¯„ä¼°å®Œæˆç»Ÿè®¡")
    print("=" * 80)
    
    if reports:
        avg_score = sum(r.metrics.overall_score for r in reports) / len(reports)
        grade_dist = {
            "A": sum(1 for r in reports if r.grade == "A"),
            "B": sum(1 for r in reports if r.grade == "B"),
            "C": sum(1 for r in reports if r.grade == "C"),
            "D": sum(1 for r in reports if r.grade == "D"),
            "F": sum(1 for r in reports if r.grade == "F")
        }
        
        print(f"\næ€»æ–‡æ¡£æ•°: {len(reports)}")
        print(f"å¹³å‡è¯„åˆ†: {avg_score:.1f}")
        print(f"é€šè¿‡ç‡: {sum(1 for r in reports if r.metrics.overall_score >= 60) / len(reports) * 100:.1f}%")
        print(f"\nç­‰çº§åˆ†å¸ƒ:")
        for grade in ["A", "B", "C", "D", "F"]:
            print(f"  {grade}: {grade_dist[grade]} ä¸ª")
    
    print("=" * 80)
    
    assessor.save_report(reports)
    
    print("\nâœ“ æ–‡æ¡£è´¨é‡è¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()
