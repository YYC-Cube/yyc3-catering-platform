#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@fileï¼šyyc3-check-document-name-content.py
@descriptionï¼šæ£€æŸ¥æ–‡æ¡£åç§°ä¸å†…å®¹å¯¹åº”å…³ç³»
@authorï¼šYYCÂ³
@versionï¼š1.0.0
@createdï¼š2025-01-30
@updatedï¼š2025-01-30
@copyrightï¼šCopyright (c) 2025 YYCÂ³
@licenseï¼šMIT
"""

import os
import re
from pathlib import Path
from typing import Dict, List, Tuple
import json


class DocumentNameContentChecker:
    """æ–‡æ¡£åç§°ä¸å†…å®¹å¯¹åº”å…³ç³»æ£€æŸ¥å™¨"""

    def __init__(self, base_path: str):
        """
        åˆå§‹åŒ–æ£€æŸ¥å™¨

        Args:
            base_path: æ–‡æ¡£åŸºç¡€è·¯å¾„
        """
        self.base_path = Path(base_path)
        self.issues = []
        self.check_results = []

    def extract_keywords_from_filename(self, filename: str) -> List[str]:
        """
        ä»æ–‡ä»¶åä¸­æå–å…³é”®è¯

        Args:
            filename: æ–‡ä»¶å

        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # ç§»é™¤æ–‡ä»¶æ‰©å±•åå’Œå‰ç¼€
        name = filename.replace('.md', '')
        # ç§»é™¤YYC3-Caterå‰ç¼€
        name = re.sub(r'^YYC3-Cater-', '', name)
        # ç§»é™¤ç¼–å·å‰ç¼€ï¼ˆå¦‚01-ã€02-1-ç­‰ï¼‰
        name = re.sub(r'^\d+(-\d+)?-', '', name)
        # ç§»é™¤åˆ†ç±»å‰ç¼€ï¼ˆå¦‚æ¶æ„ç±»-ã€æŠ€å·§ç±»-ï¼‰
        name = re.sub(r'^[æ¶æ„|æŠ€å·§]ç±»-', '', name)
        # ç§»é™¤YYC3å‰ç¼€
        name = re.sub(r'^YYC3-', '', name)

        # åˆ†å‰²å…³é”®è¯
        keywords = re.split(r'[-_]', name)
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å’Œé€šç”¨è¯
        keywords = [kw for kw in keywords if kw and kw not in ['æ–‡æ¡£', 'è®¾è®¡', 'æ¶æ„', 'è¯´æ˜', 'æŒ‡å—', 'æ‰‹å†Œ']]

        return keywords

    def extract_keywords_from_content(self, content: str) -> List[str]:
        """
        ä»æ–‡æ¡£å†…å®¹ä¸­æå–å…³é”®è¯

        Args:
            content: æ–‡æ¡£å†…å®¹

        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        keywords = []

        # ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if title_match:
            title = title_match.group(1)
            title_keywords = re.split(r'[\s-]+', title)
            keywords.extend([kw for kw in title_keywords if kw and len(kw) > 1])

        # ä»@descriptionä¸­æå–å…³é”®è¯
        desc_match = re.search(r'\*\*@description\*\*ï¼š(.+)$', content, re.MULTILINE)
        if desc_match:
            desc = desc_match.group(1)
            desc_keywords = re.split(r'[\s,ï¼Œã€]+', desc)
            keywords.extend([kw for kw in desc_keywords if kw and len(kw) > 1])

        # ä»æ–‡æ¡£ä¿¡æ¯è¡¨æ ¼ä¸­æå–å…³é”®è¯
        table_match = re.search(r'\*\*æ–‡æ¡£æ ‡é¢˜\*\*\|(.+)$', content, re.MULTILINE)
        if table_match:
            title = table_match.group(1).strip()
            title_keywords = re.split(r'[\s-]+', title)
            keywords.extend([kw for kw in title_keywords if kw and len(kw) > 1])

        # ä»ç›®å½•ä¸­æå–å…³é”®è¯
        toc_match = re.search(r'##\s+ç›®å½•\s*\n([\s\S]+?)(?=\n##|\Z)', content)
        if toc_match:
            toc = toc_match.group(1)
            toc_keywords = re.findall(r'\[(.+?)\]', toc)
            keywords.extend([kw for kw in toc_keywords if kw and len(kw) > 1])

        # å»é‡å¹¶è¿”å›
        return list(set(keywords))

    def calculate_similarity(self, name_keywords: List[str], content_keywords: List[str]) -> float:
        """
        è®¡ç®—åç§°å…³é”®è¯å’Œå†…å®¹å…³é”®è¯çš„ç›¸ä¼¼åº¦

        Args:
            name_keywords: åç§°å…³é”®è¯åˆ—è¡¨
            content_keywords: å†…å®¹å…³é”®è¯åˆ—è¡¨

        Returns:
            ç›¸ä¼¼åº¦ï¼ˆ0-1ä¹‹é—´ï¼‰
        """
        if not name_keywords or not content_keywords:
            return 0.0

        # è½¬æ¢ä¸ºå°å†™
        name_keywords = [kw.lower() for kw in name_keywords]
        content_keywords = [kw.lower() for kw in content_keywords]

        # è®¡ç®—äº¤é›†
        intersection = set(name_keywords) & set(content_keywords)

        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰
        similarity = len(intersection) / len(set(name_keywords) | set(content_keywords))

        return similarity

    def check_document(self, file_path: Path) -> Dict:
        """
        æ£€æŸ¥å•ä¸ªæ–‡æ¡£çš„åç§°ä¸å†…å®¹å¯¹åº”å…³ç³»

        Args:
            file_path: æ–‡æ¡£è·¯å¾„

        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        filename = file_path.name

        # è¯»å–æ–‡æ¡£å†…å®¹
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return {
                'filename': filename,
                'status': 'error',
                'error': str(e)
            }

        # æå–å…³é”®è¯
        name_keywords = self.extract_keywords_from_filename(filename)
        content_keywords = self.extract_keywords_from_content(content)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = self.calculate_similarity(name_keywords, content_keywords)

        # åˆ¤æ–­æ˜¯å¦åŒ¹é…
        status = 'pass' if similarity >= 0.3 else 'fail'

        # æ”¶é›†é—®é¢˜
        issues = []
        if similarity < 0.3:
            issues.append(f'ç›¸ä¼¼åº¦è¿‡ä½ï¼ˆ{similarity:.2f}ï¼‰')

        return {
            'filename': filename,
            'name_keywords': name_keywords,
            'content_keywords': content_keywords,
            'similarity': similarity,
            'status': status,
            'issues': issues
        }

    def check_all_documents(self) -> List[Dict]:
        """
        æ£€æŸ¥æ‰€æœ‰æ–‡æ¡£

        Returns:
            æ£€æŸ¥ç»“æœåˆ—è¡¨
        """
        # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
        md_files = list(self.base_path.rglob('*.md'))

        # æ’é™¤å®¡æ ¸æŠ¥å‘Šå’Œè„šæœ¬å·¥å…·
        md_files = [f for f in md_files if 'å®¡æ ¸æŠ¥å‘Š' not in str(f) and 'è„šæœ¬å·¥å…·' not in str(f)]

        print(f'æ‰¾åˆ° {len(md_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶')

        # æ£€æŸ¥æ¯ä¸ªæ–‡æ¡£
        results = []
        for file_path in md_files:
            result = self.check_document(file_path)
            results.append(result)

            # è®°å½•é—®é¢˜
            if result['status'] == 'fail':
                self.issues.append(result)

        self.check_results = results
        return results

    def generate_report(self) -> str:
        """
        ç”Ÿæˆå®¡æ ¸æŠ¥å‘Š

        Returns:
            Markdownæ ¼å¼çš„æŠ¥å‘Š
        """
        # ç»Ÿè®¡ç»“æœ
        total = len(self.check_results)
        passed = sum(1 for r in self.check_results if r['status'] == 'pass')
        failed = total - passed
        pass_rate = (passed / total * 100) if total > 0 else 0

        # ç”ŸæˆæŠ¥å‘Š
        report = f"""# YYCÂ³ æ–‡æ¡£åç§°ä¸å†…å®¹å¯¹åº”å…³ç³»å®¡æ ¸æŠ¥å‘Š

**@file**ï¼šYYC3-æ–‡æ¡£åç§°å†…å®¹å¯¹åº”å…³ç³»å®¡æ ¸æŠ¥å‘Š
**@description**ï¼šæ–‡æ¡£åç§°ä¸å†…å®¹å¯¹åº”å…³ç³»å®¡æ ¸ç»“æœå’Œæ”¹è¿›å»ºè®®
**@author**ï¼šYYCÂ³
**@version**ï¼š1.0.0
**@created**ï¼š2025-01-30
**@updated**ï¼š2025-01-30
**@status**ï¼špublished
**@tags**ï¼šå®¡æ ¸æŠ¥å‘Š,åç§°å†…å®¹å¯¹åº”,YYCÂ³

---

## ğŸ“‹ å®¡æ ¸æ¦‚è¿°

### å®¡æ ¸èŒƒå›´
- å®¡æ ¸æ–‡æ¡£æ€»æ•°ï¼š{total}ä¸ª
- å®¡æ ¸ç»´åº¦ï¼šæ–‡æ¡£åç§°ä¸å†…å®¹å¯¹åº”å…³ç³»
- å®¡æ ¸æ—¶é—´ï¼š2025-01-30

### å®¡æ ¸ç»“æœ
- âœ… é€šè¿‡æ–‡æ¡£æ•°ï¼š{passed}ä¸ª
- âŒ æœªé€šè¿‡æ–‡æ¡£æ•°ï¼š{failed}ä¸ª
- ğŸ“Š é€šè¿‡ç‡ï¼š{pass_rate:.1f}%

---

## ğŸ” è¯¦ç»†é—®é¢˜åˆ—è¡¨

### ä¸¥é‡é—®é¢˜ï¼ˆP0ï¼‰

| æ–‡æ¡£åç§° | åç§°å…³é”®è¯ | å†…å®¹å…³é”®è¯ | ç›¸ä¼¼åº¦ | ä¼˜å…ˆçº§ | å»ºè®®ä¿®å¤ |
|---------|-----------|-----------|--------|--------|---------|
"""

        # æ·»åŠ ä¸¥é‡é—®é¢˜ï¼ˆç›¸ä¼¼åº¦ < 0.1ï¼‰
        for result in self.check_results:
            if result['status'] == 'fail' and result['similarity'] < 0.1:
                report += f"| {result['filename']} | {', '.join(result['name_keywords'][:5])} | {', '.join(result['content_keywords'][:5])} | {result['similarity']:.2f} | P0 | é‡æ–°å‘½åæ–‡æ¡£æˆ–è¡¥å……å†…å®¹å…³é”®è¯ |\n"

        report += "\n### è­¦å‘Šé—®é¢˜ï¼ˆP1ï¼‰\n\n"
        report += "| æ–‡æ¡£åç§° | åç§°å…³é”®è¯ | å†…å®¹å…³é”®è¯ | ç›¸ä¼¼åº¦ | ä¼˜å…ˆçº§ | å»ºè®®ä¿®å¤ |\n"
        report += "|---------|-----------|-----------|--------|--------|---------|\n"

        # æ·»åŠ è­¦å‘Šé—®é¢˜ï¼ˆ0.1 <= ç›¸ä¼¼åº¦ < 0.3ï¼‰
        for result in self.check_results:
            if result['status'] == 'fail' and 0.1 <= result['similarity'] < 0.3:
                report += f"| {result['filename']} | {', '.join(result['name_keywords'][:5])} | {', '.join(result['content_keywords'][:5])} | {result['similarity']:.2f} | P1 | ä¼˜åŒ–æ–‡æ¡£åç§°æˆ–å†…å®¹ |\n"

        report += "\n### åˆè§„æ–‡æ¡£\n\n"
        report += "| æ–‡æ¡£åç§° | åç§°å…³é”®è¯ | å†…å®¹å…³é”®è¯ | ç›¸ä¼¼åº¦ |\n"
        report += "|---------|-----------|-----------|--------|\n"

        # æ·»åŠ åˆè§„æ–‡æ¡£
        for result in self.check_results:
            if result['status'] == 'pass':
                report += f"| {result['filename']} | {', '.join(result['name_keywords'][:5])} | {', '.join(result['content_keywords'][:5])} | {result['similarity']:.2f} |\n"

        # æ·»åŠ æ”¹è¿›å»ºè®®
        report += f"""

---

## ğŸš€ æ”¹è¿›å»ºè®®

### P0ä¼˜å…ˆçº§ï¼ˆç«‹å³è¡ŒåŠ¨ï¼‰

1. **é‡æ–°å‘½åç›¸ä¼¼åº¦è¿‡ä½çš„æ–‡æ¡£**
   - ä»»åŠ¡ï¼šä¸ºç›¸ä¼¼åº¦ä½äº0.1çš„æ–‡æ¡£é‡æ–°å‘½å
   - è´Ÿè´£äººï¼šæ–‡æ¡£ç®¡ç†å‘˜
   - æˆªæ­¢æ—¶é—´ï¼š2025-01-31
   - é¢„è®¡å·¥æ—¶ï¼š2å°æ—¶

### P1ä¼˜å…ˆçº§ï¼ˆçŸ­æœŸè¡ŒåŠ¨ï¼‰

1. **ä¼˜åŒ–æ–‡æ¡£åç§°æˆ–å†…å®¹**
   - ä»»åŠ¡ï¼šä¸ºç›¸ä¼¼åº¦åœ¨0.1-0.3ä¹‹é—´çš„æ–‡æ¡£ä¼˜åŒ–åç§°æˆ–å†…å®¹
   - è´Ÿè´£äººï¼šå„æ¨¡å—è´Ÿè´£äºº
   - æˆªæ­¢æ—¶é—´ï¼š2025-02-06
   - é¢„è®¡å·¥æ—¶ï¼š10å°æ—¶

### é•¿æœŸä¼˜åŒ–ï¼ˆæŒç»­æ”¹è¿›ï¼‰

1. **å»ºç«‹æ–‡æ¡£å‘½åè§„èŒƒ**
   - ä»»åŠ¡ï¼šåˆ¶å®šæ›´è¯¦ç»†çš„æ–‡æ¡£å‘½åè§„èŒƒ
   - è´Ÿè´£äººï¼šæ–‡æ¡£ç®¡ç†å‘˜
   - æˆªæ­¢æ—¶é—´ï¼šæŒç»­è¿›è¡Œ
   - é¢„è®¡å·¥æ—¶ï¼šæŒç»­æŠ•å…¥

2. **å»ºç«‹æ–‡æ¡£å†…å®¹æ ‡å‡†**
   - ä»»åŠ¡ï¼šåˆ¶å®šæ–‡æ¡£å†…å®¹æ ‡å‡†ï¼Œç¡®ä¿åŒ…å«å¿…è¦çš„å…³é”®è¯
   - è´Ÿè´£äººï¼šæ–‡æ¡£ç®¡ç†å‘˜
   - æˆªæ­¢æ—¶é—´ï¼šæŒç»­è¿›è¡Œ
   - é¢„è®¡å·¥æ—¶ï¼šæŒç»­æŠ•å…¥

---

## ğŸ“Š è¯„åˆ†æ ‡å‡†

| ç›¸ä¼¼åº¦èŒƒå›´ | ç­‰çº§ | è¯´æ˜ |
|-----------|------|------|
| 0.7-1.0 | A | ä¼˜ç§€ï¼Œåç§°ä¸å†…å®¹é«˜åº¦ä¸€è‡´ |
| 0.5-0.7 | B | è‰¯å¥½ï¼Œåç§°ä¸å†…å®¹åŸºæœ¬ä¸€è‡´ |
| 0.3-0.5 | C | å¯æ¥å—ï¼Œåç§°ä¸å†…å®¹éƒ¨åˆ†ä¸€è‡´ |
| 0.1-0.3 | D | éœ€è¦æ”¹è¿›ï¼Œåç§°ä¸å†…å®¹ä¸€è‡´æ€§è¾ƒä½ |
| 0.0-0.1 | F | ä¸åˆè§„ï¼Œåç§°ä¸å†…å®¹ä¸ä¸€è‡´ |

---

## ğŸ“ é™„å½•

### å®¡æ ¸æ–¹æ³•

1. **å…³é”®è¯æå–**ï¼šä»æ–‡ä»¶åå’Œæ–‡æ¡£å†…å®¹ä¸­æå–å…³é”®è¯
2. **ç›¸ä¼¼åº¦è®¡ç®—**ï¼šä½¿ç”¨Jaccardç›¸ä¼¼åº¦è®¡ç®—åç§°å…³é”®è¯å’Œå†…å®¹å…³é”®è¯çš„ç›¸ä¼¼åº¦
3. **ç»“æœåˆ¤å®š**ï¼šæ ¹æ®ç›¸ä¼¼åº¦åˆ¤æ–­æ–‡æ¡£åç§°ä¸å†…å®¹æ˜¯å¦å¯¹åº”

### å·¥å…·ä¿¡æ¯

- å·¥å…·åç§°ï¼šYYCÂ³ æ–‡æ¡£åç§°ä¸å†…å®¹å¯¹åº”å…³ç³»æ£€æŸ¥å™¨
- å·¥å…·ç‰ˆæœ¬ï¼šv1.0.0
- å·¥å…·ä½œè€…ï¼šYYCÂ³ Team

---

> ã€Œ***YanYuCloudCube***ã€
> ã€Œ***<admin@0379.email>***ã€
> ã€Œ***Words Initiate Quadrants, Language Serves as Core for the Future***ã€
> ã€Œ***All things converge in the cloud pivot; Deep stacks ignite a new era of intelligence***ã€
"""

        return report


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®åŸºç¡€è·¯å¾„
    base_path = '/Users/yanyu/yyc3-catering-platform/docs/YYC3-Cater-Platform-æ–‡æ¡£é—­ç¯'

    # åˆ›å»ºæ£€æŸ¥å™¨
    checker = DocumentNameContentChecker(base_path)

    # æ£€æŸ¥æ‰€æœ‰æ–‡æ¡£
    print('å¼€å§‹æ£€æŸ¥æ–‡æ¡£åç§°ä¸å†…å®¹å¯¹åº”å…³ç³»...')
    results = checker.check_all_documents()

    # ç”ŸæˆæŠ¥å‘Š
    print('ç”Ÿæˆå®¡æ ¸æŠ¥å‘Š...')
    report = checker.generate_report()

    # ä¿å­˜æŠ¥å‘Š
    report_path = Path(base_path) / 'YYC3-Cater-å®¡æ ¸æŠ¥å‘Š' / 'YYC3-æ–‡æ¡£åç§°å†…å®¹å¯¹åº”å…³ç³»å®¡æ ¸æŠ¥å‘Š.md'
    report_path.parent.mkdir(parents=True, exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f'å®¡æ ¸æŠ¥å‘Šå·²ä¿å­˜åˆ°ï¼š{report_path}')
    print(f'å®¡æ ¸å®Œæˆï¼šå…±æ£€æŸ¥ {len(results)} ä¸ªæ–‡æ¡£')


if __name__ == '__main__':
    main()
