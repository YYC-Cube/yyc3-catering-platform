#!/usr/bin/env python3
"""
YYCÂ³ æ–‡æ¡£çŸ¥è¯†å›¾è°±æ„å»ºå·¥å…· - ç¬¬ä¸‰é˜¶æ®µï¼ˆP2ï¼‰
åˆ†ææ–‡æ¡£å†…å®¹ï¼Œæå–å…³é”®æ¦‚å¿µï¼Œæ„å»ºæ–‡æ¡£çŸ¥è¯†å›¾è°±
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from datetime import datetime
import json
from dataclasses import dataclass, field
from collections import Counter, defaultdict


@dataclass
class DocumentNode:
    """æ–‡æ¡£èŠ‚ç‚¹"""
    file_path: str
    file_name: str
    doc_type: str
    title: str
    description: str
    keywords: List[str]
    concepts: List[str]
    references: List[str]  # å¼•ç”¨çš„å…¶ä»–æ–‡æ¡£
    referenced_by: List[str]  # è¢«å“ªäº›æ–‡æ¡£å¼•ç”¨
    category: str  # æ–‡æ¡£åˆ†ç±»
    quality_score: float = 0.0
    
    # å›¾è°±å±æ€§
    centrality: float = 0.0  # ä¸­å¿ƒæ€§
    importance: float = 0.0  # é‡è¦æ€§
    cluster: int = -1  # æ‰€å±èšç±»


@dataclass
class ConceptNode:
    """æ¦‚å¿µèŠ‚ç‚¹"""
    name: str
    category: str
    frequency: int
    documents: List[str]  # å‡ºç°åœ¨å“ªäº›æ–‡æ¡£ä¸­
    related_concepts: List[str]  # ç›¸å…³æ¦‚å¿µ
    importance: float = 0.0


@dataclass
class KnowledgeGraph:
    """çŸ¥è¯†å›¾è°±"""
    documents: Dict[str, DocumentNode] = field(default_factory=dict)
    concepts: Dict[str, ConceptNode] = field(default_factory=dict)
    edges: List[Dict] = field(default_factory=list)
    clusters: List[List[str]] = field(default_factory=list)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_documents: int = 0
    total_concepts: int = 0
    total_edges: int = 0


class DocumentKnowledgeGraphBuilder:
    """æ–‡æ¡£çŸ¥è¯†å›¾è°±æ„å»ºå™¨"""
    
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.graph = KnowledgeGraph()
        
        # å…³é”®è¯æå–æ¨¡å¼
        self.keyword_patterns = [
            r'\b[A-Z][a-zA-Z]{2,}\b',  # å¤§å†™å¼€å¤´çš„å•è¯
            r'\b(?:æ¶æ„|è®¾è®¡|å¼€å‘|æµ‹è¯•|éƒ¨ç½²|è¿ç»´|ç›‘æ§|API|æ¥å£|æœåŠ¡|æ¨¡å—|ç»„ä»¶|ç³»ç»Ÿ|å¹³å°|åº”ç”¨)\b',
            r'\b(?:AI|äººå·¥æ™ºèƒ½|æœºå™¨å­¦ä¹ |æ·±åº¦å­¦ä¹ |æ™ºèƒ½|è‡ªåŠ¨åŒ–|ä¼˜åŒ–|æ€§èƒ½|å®‰å…¨|è´¨é‡)\b',
            r'\b(?:éœ€æ±‚|è§„åˆ’|å®æ–½|è¿­ä»£|å‘å¸ƒ|ç‰ˆæœ¬|æ–‡æ¡£|è§„èŒƒ|æ ‡å‡†|æµç¨‹)\b'
        ]
        
        # æ¦‚å¿µæå–æ¨¡å¼
        self.concept_patterns = [
            r'(?:æ¶æ„|è®¾è®¡)æ¨¡å¼',
            r'(?:å¼€å‘|æµ‹è¯•|éƒ¨ç½²|è¿ç»´)æµç¨‹',
            r'(?:API|æ¥å£)è®¾è®¡',
            r'(?:æ•°æ®|ä¸šåŠ¡|æŠ€æœ¯)æ¶æ„',
            r'(?:æ€§èƒ½|å®‰å…¨|è´¨é‡)ä¿éšœ',
            r'(?:å¾®æœåŠ¡|å®¹å™¨|äº‘)éƒ¨ç½²',
            r'(?:CI/CD|DevOps)æµæ°´çº¿',
            r'(?:ç›‘æ§|å‘Šè­¦|æ—¥å¿—)ç³»ç»Ÿ',
            r'(?:éœ€æ±‚|ç”¨æˆ·|äº§å“)ç®¡ç†',
            r'(?:æ–‡æ¡£|çŸ¥è¯†)ç®¡ç†'
        ]
        
        # æ–‡æ¡£åˆ†ç±»
        self.doc_categories = {
            "æ¶æ„è®¾è®¡": ["æ¶æ„", "è®¾è®¡", "ç³»ç»Ÿ", "å¹³å°"],
            "å¼€å‘å®æ–½": ["å¼€å‘", "å®æ–½", "ç¼–ç ", "å®ç°"],
            "æµ‹è¯•éªŒè¯": ["æµ‹è¯•", "éªŒè¯", "è´¨é‡", "ç¼ºé™·"],
            "éƒ¨ç½²å‘å¸ƒ": ["éƒ¨ç½²", "å‘å¸ƒ", "è¿ç»´", "å®¹å™¨"],
            "è¿ç»´è¿è¥": ["è¿ç»´", "ç›‘æ§", "å‘Šè­¦", "æ—¥å¿—"],
            "éœ€æ±‚è§„åˆ’": ["éœ€æ±‚", "è§„åˆ’", "äº§å“", "ç”¨æˆ·"],
            "ç”¨æˆ·æŒ‡å—": ["æŒ‡å—", "æ‰‹å†Œ", "æ•™ç¨‹", "å…¥é—¨"],
            "å½’ç±»è¿­ä»£": ["è¿­ä»£", "ç‰ˆæœ¬", "æ›´æ–°", "å˜æ›´"]
        }
    
    def extract_title(self, content: str) -> str:
        """æå–æ–‡æ¡£æ ‡é¢˜"""
        match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if match:
            return match.group(1).strip()
        return ""
    
    def extract_description(self, content: str) -> str:
        """æå–æ–‡æ¡£æè¿°"""
        # æŸ¥æ‰¾ @description
        match = re.search(r'@description\s*[:ï¼š]?\s*(.+?)(?:\n|$)', content)
        if match:
            return match.group(1).strip()
        
        # æŸ¥æ‰¾æè¿°ç« èŠ‚
        match = re.search(r'##\s*(?:æè¿°|è¯´æ˜|æ¦‚è¿°|ç®€ä»‹)\s*\n\s*(.+?)(?:\n##|\n\n|$)', content, re.DOTALL)
        if match:
            return match.group(1).strip()[:200]  # é™åˆ¶é•¿åº¦
        
        return ""
    
    def extract_keywords(self, content: str) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = []
        
        for pattern in self.keyword_patterns:
            matches = re.findall(pattern, content)
            keywords.extend(matches)
        
        # ç»Ÿè®¡è¯é¢‘
        keyword_freq = Counter(keywords)
        
        # è¿”å›å‰10ä¸ªé«˜é¢‘å…³é”®è¯
        return [kw for kw, _ in keyword_freq.most_common(10)]
    
    def extract_concepts(self, content: str) -> List[str]:
        """æå–æ¦‚å¿µ"""
        concepts = []
        
        for pattern in self.concept_patterns:
            matches = re.findall(pattern, content)
            concepts.extend(matches)
        
        # å»é‡
        return list(set(concepts))
    
    def extract_references(self, content: str, current_file: str) -> List[str]:
        """æå–æ–‡æ¡£å¼•ç”¨"""
        references = []
        
        # æŸ¥æ‰¾Markdowné“¾æ¥
        links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
        for text, url in links:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡æ¡£é“¾æ¥
            if url.endswith('.md') or url.endswith('.MD'):
                references.append(text)
        
        # æŸ¥æ‰¾æ–‡æ¡£åç§°å¼•ç”¨
        doc_refs = re.findall(r'YYC3-[^:\s\]]+', content)
        references.extend(doc_refs)
        
        # å»é‡
        return list(set(references))
    
    def classify_document(self, file_name: str, content: str) -> str:
        """åˆ†ç±»æ–‡æ¡£"""
        file_lower = file_name.lower()
        content_lower = content.lower()
        
        for category, keywords in self.doc_categories.items():
            for keyword in keywords:
                if keyword in file_lower or keyword in content_lower:
                    return category
        
        return "å…¶ä»–"
    
    def load_quality_scores(self) -> Dict[str, float]:
        """åŠ è½½æ–‡æ¡£è´¨é‡è¯„åˆ†"""
        quality_file = self.base_path / "YYC3-Cater-å®¡æ ¸æŠ¥å‘Š" / "YYC3-æ–‡æ¡£è´¨é‡è¯„ä¼°æŠ¥å‘Š.json"
        
        if not quality_file.exists():
            return {}
        
        with open(quality_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        scores = {}
        for report in data.get("reports", []):
            scores[report["file_name"]] = report["metrics"]["overall_score"]
        
        return scores
    
    def build_document_nodes(self) -> Dict[str, DocumentNode]:
        """æ„å»ºæ–‡æ¡£èŠ‚ç‚¹"""
        documents = {}
        quality_scores = self.load_quality_scores()
        
        for file in self.base_path.rglob("*.md"):
            if file.name == "README.md":
                continue
            
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                title = self.extract_title(content)
                description = self.extract_description(content)
                keywords = self.extract_keywords(content)
                concepts = self.extract_concepts(content)
                references = self.extract_references(content, file.name)
                category = self.classify_document(file.name, content)
                quality_score = quality_scores.get(file.name, 0.0)
                
                node = DocumentNode(
                    file_path=str(file),
                    file_name=file.name,
                    doc_type="architecture" if "æ¶æ„ç±»" in file.name else "technique",
                    title=title,
                    description=description,
                    keywords=keywords,
                    concepts=concepts,
                    references=references,
                    referenced_by=[],
                    category=category,
                    quality_score=quality_score
                )
                
                documents[file.name] = node
                print(f"âœ“ å·²å¤„ç†: {file.name}")
                
            except Exception as e:
                print(f"âœ— å¤„ç†å¤±è´¥: {file.name} - {e}")
        
        return documents
    
    def build_concept_nodes(self, documents: Dict[str, DocumentNode]) -> Dict[str, ConceptNode]:
        """æ„å»ºæ¦‚å¿µèŠ‚ç‚¹"""
        concepts = defaultdict(lambda: {
            "name": "",
            "category": "",
            "frequency": 0,
            "documents": [],
            "related_concepts": []
        })
        
        # ç»Ÿè®¡æ¦‚å¿µé¢‘ç‡
        for doc_name, doc_node in documents.items():
            for concept in doc_node.concepts:
                concepts[concept]["name"] = concept
                concepts[concept]["frequency"] += 1
                concepts[concept]["documents"].append(doc_name)
        
        # æ„å»ºæ¦‚å¿µèŠ‚ç‚¹
        concept_nodes = {}
        for concept_name, concept_data in concepts.items():
            # åˆ†ç±»æ¦‚å¿µ
            category = "æŠ€æœ¯"
            if "æ¶æ„" in concept_name:
                category = "æ¶æ„"
            elif "å¼€å‘" in concept_name or "æµ‹è¯•" in concept_name:
                category = "å¼€å‘"
            elif "éƒ¨ç½²" in concept_name or "è¿ç»´" in concept_name:
                category = "è¿ç»´"
            elif "éœ€æ±‚" in concept_name or "ç”¨æˆ·" in concept_name:
                category = "äº§å“"
            
            # è®¡ç®—é‡è¦æ€§ï¼ˆé¢‘ç‡ * æ–‡æ¡£è´¨é‡å¹³å‡åˆ†ï¼‰
            doc_scores = [documents[doc].quality_score for doc in concept_data["documents"]]
            avg_score = sum(doc_scores) / len(doc_scores) if doc_scores else 0
            importance = concept_data["frequency"] * avg_score / 100
            
            concept_node = ConceptNode(
                name=concept_name,
                category=category,
                frequency=concept_data["frequency"],
                documents=concept_data["documents"],
                related_concepts=[],
                importance=importance
            )
            
            concept_nodes[concept_name] = concept_node
        
        return concept_nodes
    
    def build_edges(self, documents: Dict[str, DocumentNode]) -> List[Dict]:
        """æ„å»ºè¾¹"""
        edges = []
        
        # æ„å»ºæ–‡æ¡£å¼•ç”¨è¾¹
        for doc_name, doc_node in documents.items():
            for ref in doc_node.references:
                # æŸ¥æ‰¾è¢«å¼•ç”¨çš„æ–‡æ¡£
                for other_name, other_node in documents.items():
                    if ref in other_name or ref in other_node.title:
                        edges.append({
                            "source": doc_name,
                            "target": other_name,
                            "type": "reference",
                            "weight": 1.0
                        })
                        
                        # è®°å½•è¢«å¼•ç”¨å…³ç³»
                        if doc_name not in other_node.referenced_by:
                            other_node.referenced_by.append(doc_name)
                        break
        
        # æ„å»ºæ¦‚å¿µå…³è”è¾¹
        for doc_name, doc_node in documents.items():
            for concept in doc_node.concepts:
                for other_name, other_node in documents.items():
                    if doc_name == other_name:
                        continue
                    
                    # å¦‚æœä¸¤ä¸ªæ–‡æ¡£å…±äº«æ¦‚å¿µï¼Œå»ºç«‹å…³è”
                    shared_concepts = set(doc_node.concepts) & set(other_node.concepts)
                    if shared_concepts:
                        edges.append({
                            "source": doc_name,
                            "target": other_name,
                            "type": "concept",
                            "weight": len(shared_concepts),
                            "concepts": list(shared_concepts)
                        })
        
        return edges
    
    def calculate_centrality(self, documents: Dict[str, DocumentNode], edges: List[Dict]):
        """è®¡ç®—ä¸­å¿ƒæ€§"""
        # è®¡ç®—åº¦ä¸­å¿ƒæ€§ï¼ˆå…¥åº¦+å‡ºåº¦ï¼‰
        for doc_name, doc_node in documents.items():
            in_degree = len(doc_node.referenced_by)
            out_degree = len(doc_node.references)
            doc_node.centrality = in_degree * 2 + out_degree  # å…¥åº¦æƒé‡æ›´é«˜
    
    def calculate_importance(self, documents: Dict[str, DocumentNode]):
        """è®¡ç®—é‡è¦æ€§"""
        max_centrality = max(doc.centrality for doc in documents.values()) if documents else 1
        max_quality = max(doc.quality_score for doc in documents.values()) if documents else 1
        
        for doc_node in documents.values():
            # é‡è¦æ€§ = ä¸­å¿ƒæ€§(40%) + è´¨é‡è¯„åˆ†(40%) + å¼•ç”¨æ•°(20%)
            centrality_score = doc_node.centrality / max_centrality if max_centrality > 0 else 0
            quality_score = doc_node.quality_score / max_quality if max_quality > 0 else 0
            reference_score = len(doc_node.referenced_by) / len(documents) if documents else 0
            
            doc_node.importance = centrality_score * 0.4 + quality_score * 0.4 + reference_score * 0.2
    
    def build_graph(self) -> KnowledgeGraph:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        print("=" * 80)
        print("æ„å»ºæ–‡æ¡£çŸ¥è¯†å›¾è°±")
        print("=" * 80)
        print()
        
        # æ„å»ºæ–‡æ¡£èŠ‚ç‚¹
        print("æ­¥éª¤1: æ„å»ºæ–‡æ¡£èŠ‚ç‚¹...")
        documents = self.build_document_nodes()
        self.graph.documents = documents
        self.graph.total_documents = len(documents)
        print(f"âœ“ å·²æ„å»º {len(documents)} ä¸ªæ–‡æ¡£èŠ‚ç‚¹\n")
        
        # æ„å»ºæ¦‚å¿µèŠ‚ç‚¹
        print("æ­¥éª¤2: æ„å»ºæ¦‚å¿µèŠ‚ç‚¹...")
        concepts = self.build_concept_nodes(documents)
        self.graph.concepts = concepts
        self.graph.total_concepts = len(concepts)
        print(f"âœ“ å·²æ„å»º {len(concepts)} ä¸ªæ¦‚å¿µèŠ‚ç‚¹\n")
        
        # æ„å»ºè¾¹
        print("æ­¥éª¤3: æ„å»ºè¾¹...")
        edges = self.build_edges(documents)
        self.graph.edges = edges
        self.graph.total_edges = len(edges)
        print(f"âœ“ å·²æ„å»º {len(edges)} æ¡è¾¹\n")
        
        # è®¡ç®—ä¸­å¿ƒæ€§
        print("æ­¥éª¤4: è®¡ç®—ä¸­å¿ƒæ€§...")
        self.calculate_centrality(documents, edges)
        print("âœ“ å·²è®¡ç®—ä¸­å¿ƒæ€§\n")
        
        # è®¡ç®—é‡è¦æ€§
        print("æ­¥éª¤5: è®¡ç®—é‡è¦æ€§...")
        self.calculate_importance(documents)
        print("âœ“ å·²è®¡ç®—é‡è¦æ€§\n")
        
        return self.graph
    
    def save_graph(self, output_dir: Path):
        """ä¿å­˜çŸ¥è¯†å›¾è°±"""
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜JSONæ ¼å¼
        json_file = output_dir / f"YYC3-æ–‡æ¡£çŸ¥è¯†å›¾è°±_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        graph_data = {
            "timestamp": datetime.now().isoformat(),
            "statistics": {
                "total_documents": self.graph.total_documents,
                "total_concepts": self.graph.total_concepts,
                "total_edges": self.graph.total_edges
            },
            "documents": [
                {
                    "name": node.file_name,
                    "title": node.title,
                    "category": node.category,
                    "keywords": node.keywords,
                    "concepts": node.concepts,
                    "quality_score": node.quality_score,
                    "centrality": node.centrality,
                    "importance": node.importance,
                    "references": node.references,
                    "referenced_by": node.referenced_by
                }
                for node in self.graph.documents.values()
            ],
            "concepts": [
                {
                    "name": node.name,
                    "category": node.category,
                    "frequency": node.frequency,
                    "documents": node.documents,
                    "importance": node.importance
                }
                for node in self.graph.concepts.values()
            ],
            "edges": self.graph.edges
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(graph_data, f, ensure_ascii=False, indent=2)
        
        print(f"JSONå›¾è°±å·²ä¿å­˜åˆ°: {json_file}")
        
        # ä¿å­˜å¯è§†åŒ–æ•°æ®ï¼ˆç”¨äºD3.jsç­‰å¯è§†åŒ–åº“ï¼‰
        self.save_visualization_data(output_dir)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(output_dir)
    
    def save_visualization_data(self, output_dir: Path):
        """ä¿å­˜å¯è§†åŒ–æ•°æ®"""
        vis_file = output_dir / f"YYC3-æ–‡æ¡£çŸ¥è¯†å›¾è°±å¯è§†åŒ–_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # æ„å»ºèŠ‚ç‚¹å’Œè¾¹
        nodes = []
        links = []
        
        # æ·»åŠ æ–‡æ¡£èŠ‚ç‚¹
        for doc_name, doc_node in self.graph.documents.items():
            nodes.append({
                "id": doc_name,
                "type": "document",
                "title": doc_node.title,
                "category": doc_node.category,
                "size": 10 + doc_node.importance * 30,
                "color": self.get_category_color(doc_node.category),
                "importance": doc_node.importance,
                "quality": doc_node.quality_score
            })
        
        # æ·»åŠ æ¦‚å¿µèŠ‚ç‚¹
        for concept_name, concept_node in self.graph.concepts.items():
            nodes.append({
                "id": f"concept_{concept_name}",
                "type": "concept",
                "title": concept_name,
                "category": concept_node.category,
                "size": 5 + concept_node.importance * 20,
                "color": self.get_concept_color(concept_node.category),
                "importance": concept_node.importance,
                "frequency": concept_node.frequency
            })
        
        # æ·»åŠ è¾¹
        for edge in self.graph.edges:
            links.append({
                "source": edge["source"],
                "target": edge["target"],
                "type": edge["type"],
                "weight": edge["weight"]
            })
        
        vis_data = {
            "nodes": nodes,
            "links": links
        }
        
        with open(vis_file, 'w', encoding='utf-8') as f:
            json.dump(vis_data, f, ensure_ascii=False, indent=2)
        
        print(f"å¯è§†åŒ–æ•°æ®å·²ä¿å­˜åˆ°: {vis_file}")
    
    def get_category_color(self, category: str) -> str:
        """è·å–åˆ†ç±»é¢œè‰²"""
        colors = {
            "æ¶æ„è®¾è®¡": "#FF6B6B",
            "å¼€å‘å®æ–½": "#4ECDC4",
            "æµ‹è¯•éªŒè¯": "#45B7D1",
            "éƒ¨ç½²å‘å¸ƒ": "#96CEB4",
            "è¿ç»´è¿è¥": "#FFEAA7",
            "éœ€æ±‚è§„åˆ’": "#DDA0DD",
            "ç”¨æˆ·æŒ‡å—": "#98D8C8",
            "å½’ç±»è¿­ä»£": "#F7DC6F",
            "å…¶ä»–": "#BDC3C7"
        }
        return colors.get(category, "#BDC3C7")
    
    def get_concept_color(self, category: str) -> str:
        """è·å–æ¦‚å¿µé¢œè‰²"""
        colors = {
            "æ¶æ„": "#E74C3C",
            "å¼€å‘": "#3498DB",
            "è¿ç»´": "#2ECC71",
            "äº§å“": "#9B59B6",
            "æŠ€æœ¯": "#F39C12"
        }
        return colors.get(category, "#95A5A6")
    
    def generate_markdown_report(self, output_dir: Path):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        md_file = output_dir / f"YYC3-æ–‡æ¡£çŸ¥è¯†å›¾è°±æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        # æŒ‰é‡è¦æ€§æ’åºæ–‡æ¡£
        sorted_docs = sorted(
            self.graph.documents.values(),
            key=lambda x: x.importance,
            reverse=True
        )
        
        # æŒ‰é‡è¦æ€§æ’åºæ¦‚å¿µ
        sorted_concepts = sorted(
            self.graph.concepts.values(),
            key=lambda x: x.importance,
            reverse=True
        )
        
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# YYCÂ³ æ–‡æ¡£çŸ¥è¯†å›¾è°±æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ğŸ“Š å›¾è°±ç»Ÿè®¡\n\n")
            f.write(f"- **æ–‡æ¡£èŠ‚ç‚¹æ•°**: {self.graph.total_documents}\n")
            f.write(f"- **æ¦‚å¿µèŠ‚ç‚¹æ•°**: {self.graph.total_concepts}\n")
            f.write(f"- **è¾¹æ•°**: {self.graph.total_edges}\n\n")
            
            f.write("## ğŸ“„ é‡è¦æ–‡æ¡£TOP20\n\n")
            f.write("| æ’å | æ–‡æ¡£åç§° | åˆ†ç±» | é‡è¦æ€§ | è´¨é‡è¯„åˆ† | ä¸­å¿ƒæ€§ |\n")
            f.write("|------|---------|------|--------|---------|--------|\n")
            for i, doc in enumerate(sorted_docs[:20], 1):
                f.write(f"| {i} | {doc.file_name[:30]} | {doc.category} | {doc.importance:.3f} | {doc.quality_score:.1f} | {doc.centrality:.0f} |\n")
            f.write("\n")
            
            f.write("## ğŸ’¡ é‡è¦æ¦‚å¿µTOP20\n\n")
            f.write("| æ’å | æ¦‚å¿µåç§° | åˆ†ç±» | é¢‘ç‡ | é‡è¦æ€§ | æ–‡æ¡£æ•° |\n")
            f.write("|------|---------|------|------|--------|--------|\n")
            for i, concept in enumerate(sorted_concepts[:20], 1):
                f.write(f"| {i} | {concept.name} | {concept.category} | {concept.frequency} | {concept.importance:.3f} | {len(concept.documents)} |\n")
            f.write("\n")
            
            f.write("## ğŸ”— æ–‡æ¡£å¼•ç”¨å…³ç³»\n\n")
            f.write("### å¼•ç”¨æœ€å¤šçš„æ–‡æ¡£TOP10\n\n")
            most_referenced = sorted(
                self.graph.documents.values(),
                key=lambda x: len(x.referenced_by),
                reverse=True
            )[:10]
            f.write("| æ’å | æ–‡æ¡£åç§° | è¢«å¼•ç”¨æ¬¡æ•° | å¼•ç”¨è€… |\n")
            f.write("|------|---------|-----------|--------|\n")
            for i, doc in enumerate(most_referenced, 1):
                ref_by = ", ".join(doc.referenced_by[:3])
                if len(doc.referenced_by) > 3:
                    ref_by += f" ç­‰{len(doc.referenced_by)}ä¸ª"
                f.write(f"| {i} | {doc.file_name[:30]} | {len(doc.referenced_by)} | {ref_by} |\n")
            f.write("\n")
            
            f.write("### å¼•ç”¨æœ€å¤šçš„æ–‡æ¡£TOP10\n\n")
            most_referencing = sorted(
                self.graph.documents.values(),
                key=lambda x: len(x.references),
                reverse=True
            )[:10]
            f.write("| æ’å | æ–‡æ¡£åç§° | å¼•ç”¨æ¬¡æ•° | è¢«å¼•ç”¨æ–‡æ¡£ |\n")
            f.write("|------|---------|---------|----------|\n")
            for i, doc in enumerate(most_referencing, 1):
                refs = ", ".join(doc.references[:3])
                if len(doc.references) > 3:
                    refs += f" ç­‰{len(doc.references)}ä¸ª"
                f.write(f"| {i} | {doc.file_name[:30]} | {len(doc.references)} | {refs} |\n")
            f.write("\n")
            
            f.write("## ğŸ“‚ æ–‡æ¡£åˆ†ç±»ç»Ÿè®¡\n\n")
            category_stats = defaultdict(int)
            for doc in self.graph.documents.values():
                category_stats[doc.category] += 1
            
            f.write("| åˆ†ç±» | æ–‡æ¡£æ•° | å æ¯” |\n")
            f.write("|------|--------|------|\n")
            total = len(self.graph.documents)
            for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = count / total * 100 if total > 0 else 0
                f.write(f"| {category} | {count} | {percentage:.1f}% |\n")
            f.write("\n")
            
            f.write("## ğŸ“ˆ è´¨é‡ä¸é‡è¦æ€§åˆ†æ\n\n")
            f.write("### é«˜è´¨é‡é«˜é‡è¦æ€§æ–‡æ¡£\n\n")
            high_quality_high_importance = [
                doc for doc in self.graph.documents.values()
                if doc.quality_score >= 80 and doc.importance >= 0.5
            ]
            for doc in high_quality_high_importance[:10]:
                f.write(f"- **{doc.file_name}**: è´¨é‡{doc.quality_score:.1f}, é‡è¦æ€§{doc.importance:.3f}\n")
            f.write("\n")
            
            f.write("### ä½è´¨é‡é«˜é‡è¦æ€§æ–‡æ¡£ï¼ˆéœ€ä¼˜å…ˆæ”¹è¿›ï¼‰\n\n")
            low_quality_high_importance = [
                doc for doc in self.graph.documents.values()
                if doc.quality_score < 70 and doc.importance >= 0.5
            ]
            if low_quality_high_importance:
                for doc in low_quality_high_importance[:10]:
                    f.write(f"- **{doc.file_name}**: è´¨é‡{doc.quality_score:.1f}, é‡è¦æ€§{doc.importance:.3f}\n")
            else:
                f.write("æ— ä½è´¨é‡é«˜é‡è¦æ€§æ–‡æ¡£\n")
            f.write("\n")
        
        print(f"MarkdownæŠ¥å‘Šå·²ä¿å­˜åˆ°: {md_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='YYCÂ³ æ–‡æ¡£çŸ¥è¯†å›¾è°±æ„å»ºå·¥å…·')
    parser.add_argument('--base-path', type=str,
                       default='/Users/yanyu/yyc3-catering-platform/docs/YYC3-Cater-Platform-æ–‡æ¡£é—­ç¯',
                       help='æ–‡æ¡£æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', type=str,
                       default='/Users/yanyu/yyc3-catering-platform/docs/YYC3-Cater-Platform-æ–‡æ¡£é—­ç¯/YYC3-Cater-å®¡æ ¸æŠ¥å‘Š',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("YYCÂ³ æ–‡æ¡£çŸ¥è¯†å›¾è°±æ„å»ºå·¥å…· - ç¬¬ä¸‰é˜¶æ®µï¼ˆP2ï¼‰")
    print("=" * 80)
    print(f"æ–‡æ¡£ç›®å½•: {args.base_path}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print("=" * 80)
    print()
    
    builder = DocumentKnowledgeGraphBuilder(args.base_path)
    builder.build_graph()
    
    print()
    print("=" * 80)
    print("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
    print("=" * 80)
    print(f"\næ–‡æ¡£èŠ‚ç‚¹: {builder.graph.total_documents}")
    print(f"æ¦‚å¿µèŠ‚ç‚¹: {builder.graph.total_concepts}")
    print(f"è¾¹: {builder.graph.total_edges}")
    print("=" * 80)
    
    builder.save_graph(Path(args.output_dir))
    
    print("\nâœ“ æ–‡æ¡£çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")


if __name__ == "__main__":
    main()
